{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWIJp4JTx1td"
      },
      "source": [
        "# Multinomial Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pkH1OvWXx0og"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm_notebook\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class MultinomialNaiveBayes:\n",
        "    def fit(self, X, y):\n",
        "        # Calculate the number of classes and store the class labels\n",
        "        self.classes = np.unique(y)\n",
        "        n_classes = len(self.classes)\n",
        "        \n",
        "        # Calculate the number of samples and features\n",
        "        n_samples, n_features = X.shape\n",
        "        \n",
        "        # Calculate the class priors\n",
        "        self.priors = np.zeros(n_classes)\n",
        "        for i in range(n_classes):\n",
        "            #self.priors[i] = \n",
        "            pass\n",
        "        # Calculate the class-conditional feature probabilities\n",
        "        self.counts = np.zeros((n_classes, n_features))\n",
        "        for i in range(n_classes):\n",
        "            X_class = X[y == self.classes[i],:]\n",
        "            self.counts[i,:] = np.sum(X_class, axis=0) + 1\n",
        "        self.counts /= np.sum(self.counts, axis=1).reshape(-1, 1) + n_features\n",
        "        \n",
        "    def predict(self, X):\n",
        "        # Calculate the log probability of each class for each sample\n",
        "        #log_probs = \n",
        "        \n",
        "        # Return the class with the highest log probability for each sample\n",
        "        return self.classes[np.argmax(log_probs, axis=1)]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cp5ohemuyLvm"
      },
      "source": [
        "In the multinomial Naive Bayes model, each document is represented as a bag of words and the number of occurrences of each word is used as a feature.\n",
        "\n",
        "Given a set of $m$ training documents, $C$ classes, and a vocabulary of $n$ words, let $x$ be a new document represented as a bag of words, where $x_i$ is the count of the i-th word in the vocabulary in the document. The goal is to find the class $y$ that maximizes the posterior probability, $P(y|x)$, using Bayes' Theorem:\n",
        "\n",
        ">$P(y|x) = \\frac{P(x|y)P(y)}{P(x)}$\n",
        "\n",
        "Here, $P(y)$ is the prior probability of the class, which can be estimated as the fraction of documents in the training set that belong to class y. $P(x|y)$ is the likelihood of the document given the class, which can be estimated as the product of the probabilities of each word in the vocabulary given the class. $P(x)$ is the normalizing constant, which is the same for all classes and can be ignored for the purposes of estimation.\n",
        "\n",
        "Using the multinomial distribution, the likelihood can be written as:\n",
        "\n",
        ">$P(x|y) = \\prod_{i=1}^n P(x_i|y)$\n",
        "\n",
        "Where $P(x_i|y)$ is the probability of observing word $i$ in a document given class $y$, which can be estimated as the count of word $i$ in class $y$ divided by the total count of all words in class $y$.\n",
        "\n",
        "Finally, taking the log of the posterior probabilities makes the calculation easier and allows us to find the MAP estimate by simply taking the maximum value:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\log P(y|x) &= \\log \\frac{P(x|y)P(y)}{P(x)} \\\\\n",
        "&= \\log P(x|y) + \\log P(y) - \\log P(x) \\\\\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "For prediction's we can ignore $\\log P(x)$ term and report the y that has highest $\\log P(y = i|x)$, where $i = {1,\\dots,c}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skTv0AJO5HwF"
      },
      "source": [
        "Above implementation has two missing parts. Let's develop it step by step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kS2NM7s_5l19"
      },
      "source": [
        "**Q1: PART A [5 points]**\n",
        "\n",
        "Before writing any code, consider the following toy input. After fitting the dataset, what should be the value in `self.priors` ?\n",
        "\n",
        "Note: you need to just write down the answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R668ansf6SOD"
      },
      "source": [
        "Solution: The self.priors will be 1/2, 1/2. Because as we can see the y has length 4, and there are two 0, and two 1. So the prior should be 2/4 and 2/4, which is 1/2, 1/2\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PIthJ5MMx-4k"
      },
      "outputs": [],
      "source": [
        "# Example usage\n",
        "\n",
        "#here we assume n = 4, and each X, say [1,1,2,0] represents the corresponding counts of each of those words in that sentence.\n",
        "X = np.array([[1, 1, 2, 0], [2, 1, 1, 0], [0, 2, 1, 2], [1, 1, 0, 2]])\n",
        "# we have two classes \n",
        "y = np.array([0, 1, 0, 1])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vG_d4FoO6UUb"
      },
      "source": [
        "\n",
        "**Q1: PART B [15 points]**\n",
        "\n",
        "In the `fit` function fill the missing line by uncommenting `self.priors[i] = ` and also remove `pass` after doing so. \n",
        "\n",
        "Note: Make edits in the below template.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LK2X6tONuICl"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "g0z_u9VG6hoo"
      },
      "outputs": [],
      "source": [
        "class MultinomialNaiveBayes:\n",
        "    def fit(self, X, y):\n",
        "        # Calculate the number of classes and store the class labels\n",
        "        self.classes = np.unique(y)\n",
        "        n_classes = len(self.classes)\n",
        "        \n",
        "        # Calculate the number of samples and features\n",
        "        n_samples, n_features = X.shape\n",
        "        \n",
        "        # Calculate the class priors\n",
        "        unique, counts = np.unique(y, return_counts=True)\n",
        "        dic = dict(zip(unique, counts))\n",
        "        self.priors = np.zeros(n_classes)\n",
        "        for i in range(n_classes):\n",
        "            # self.priors[i] = np.log(self.classes[i]/np.sum(self.classes))\n",
        "            self.priors[i] = dic.get(i)/len(y)\n",
        "            # pass\n",
        "        # Calculate the class-conditional feature probabilities\n",
        "        self.counts = np.zeros((n_classes, n_features))\n",
        "        for i in range(n_classes):\n",
        "            X_class = X[y == self.classes[i],:]\n",
        "            self.counts[i,:] = np.sum(X_class, axis=0) + 1\n",
        "        self.counts /= np.sum(self.counts, axis=1).reshape(-1, 1) + n_features\n",
        "        \n",
        "        # test\n",
        "        # print()\n",
        "    def predict(self, X):\n",
        "        # Calculate the log probability of each class for each sample\n",
        "        #log_probs = \n",
        "        \n",
        "        # Return the class with the highest log probability for each sample\n",
        "        return self.classes[np.argmax(log_probs, axis=1)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "FsdnL31MD7PW"
      },
      "outputs": [],
      "source": [
        "nb = MultinomialNaiveBayes()\n",
        "nb.fit(X, y)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NX4IzXoyjls"
      },
      "source": [
        "Here fit calculates the p(y) and also stores two sets of parameters p(x|y) for y=0 and y=1. Let's check these before proceeding. (run the below cells)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "lYakGrx7yiZ2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2de5855c-c7e0-467c-e319-b6003318b963"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.5, 0.5])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "nb.priors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "zgKXy9J8y5mJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43634630-35fe-44f4-8063-bf952e921e02"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "nb.counts.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "MXt-esstzlBQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31ade0ab-5a1b-474a-8903-a55159193a7a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.11764706, 0.23529412, 0.23529412, 0.17647059],\n",
              "       [0.25      , 0.1875    , 0.125     , 0.1875    ]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "nb.counts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I61FIcvuyjDi"
      },
      "source": [
        "**Q1: PART C [15 points]**\n",
        "\n",
        "Complete the `log_probs = ` line in predict function. (code block is pasted below)\n",
        "\n",
        "Note: If you can't implement in a single line you are free to write it in multiple lines. If your implementation is correct below `assert` statement should run with no error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "_5hLOnmB0yDV"
      },
      "outputs": [],
      "source": [
        "from numpy.core.memmap import dtype\n",
        "from scipy.stats import binom, norm\n",
        "\n",
        "# class MultinomialNaiveBayes:\n",
        "#     def fit(self, X, y):\n",
        "#         # Calculate the number of classes and store the class labels\n",
        "#         self.classes = np.unique(y)\n",
        "#         n_classes = len(self.classes)\n",
        "        \n",
        "#         # Calculate the number of samples and features\n",
        "#         n_samples, n_features = X.shape\n",
        "        \n",
        "#         # Calculate the class priors\n",
        "#         self.priors = np.zeros(n_classes)\n",
        "#         for i in range(n_classes):\n",
        "#             # self.priors[i] = self.classes[i]/np.sum(self.classes)\n",
        "#             self.priors[i] = self.classes[i]/np.sum(self.classes)\n",
        "#             # pass\n",
        "#         # Calculate the class-conditional feature probabilities\n",
        "#         self.counts = np.zeros((n_classes, n_features))\n",
        "#         for i in range(n_classes):\n",
        "#             X_class = X[y == self.classes[i],:]\n",
        "#             self.counts[i,:] = np.sum(X_class, axis=0) + 1\n",
        "#             # print(self.counts[i,:])\n",
        "#         self.counts /= np.sum(self.counts, axis=1).reshape(-1, 1) + n_features\n",
        "class MultinomialNaiveBayes:\n",
        "    def fit(self, X, y):\n",
        "        # Calculate the number of classes and store the class labels\n",
        "        self.classes = np.unique(y)\n",
        "        n_classes = len(self.classes)\n",
        "        \n",
        "        # Calculate the number of samples and features\n",
        "        n_samples, n_features = X.shape\n",
        "        \n",
        "        # Calculate the class priors\n",
        "        unique, counts = np.unique(y, return_counts=True)\n",
        "        dic = dict(zip(unique, counts))\n",
        "        self.priors = np.zeros(n_classes)\n",
        "        for i in range(n_classes):\n",
        "            # self.priors[i] = np.log(self.classes[i]/np.sum(self.classes))\n",
        "            self.priors[i] = dic.get(i)/len(y)\n",
        "            # pass\n",
        "        # Calculate the class-conditional feature probabilities\n",
        "        self.counts = np.zeros((n_classes, n_features))\n",
        "        for i in range(n_classes):\n",
        "            X_class = X[y == self.classes[i],:]\n",
        "            self.counts[i,:] = np.sum(X_class, axis=0) + 1\n",
        "        self.counts /= np.sum(self.counts, axis=1).reshape(-1, 1) + n_features\n",
        "        \n",
        "        # self.count was [2. 4. 4. 3.], [4. 3. 2. 3.], then each value need to divide the sum + 4\n",
        "    def predict(self, X):\n",
        "      # returns a list of class labels for each x in X.\n",
        "      # Calculate the log probability of each class for each sample\n",
        "      # 0 has 9 words, 1 has 8 words\n",
        "      # 2 classes\n",
        "      \n",
        "      '''\n",
        "      \n",
        "      probs = self.counts.reshape(-1,len(self.counts))\n",
        "      log_probs = self.counts.reshape(-1,len(self.counts))\n",
        "      for n in range(len(log_probs)):\n",
        "        for i in range(len(log_probs[n])):\n",
        "          log_probs[n][i] = 1\n",
        "      '''\n",
        "\n",
        "      log_probs = np.dot(np.log(self.counts), X.T) + np.log(self.priors).reshape(-1, 1)\n",
        "      log_probs = log_probs.T\n",
        "      \n",
        "      '''\n",
        "      n_class, n_theta = self.counts.shape\n",
        "      n_samples, n_features = X.shape\n",
        "      log_probs = []\n",
        "      likeli = []\n",
        "      for i in range(n_class):\n",
        "        for j in range(n_samples):\n",
        "          # likeli.append(np.log(np.prod(np.power(self.counts[i,:], X[j,:]))))\n",
        "          likeli.append(np.log(np.prod(np.power(self.counts[i,:], X[j,:]))))\n",
        "          # print(i, j)\n",
        "      for i in range(len(likeli)//2):\n",
        "        log_probs.append([likeli[i] + np.log(self.priors[0]), likeli[i+n_samples]] + np.log(self.priors[1]))\n",
        "      \n",
        "      # print(likeli)\n",
        "\n",
        "      '''\n",
        "        \n",
        "      print(log_probs)\n",
        "\n",
        "\n",
        "        \n",
        "      # Return the class with the highest log probability for each sample\n",
        "      return self.classes[np.argmax(log_probs, axis=1)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "00SpKDb88Nei",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5251cf49-cbb8-41bb-ef49-e4127660caa7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-7.17397029 -7.91230106]\n",
            " [-7.86711747 -7.21915388]\n",
            " [-8.50310624 -9.46849446]\n",
            " [-7.74933444 -7.10137084]]\n",
            "[0 1 0 1]\n",
            "[[-7.17397029 -7.91230106]\n",
            " [-7.86711747 -7.21915388]\n",
            " [-8.50310624 -9.46849446]\n",
            " [-7.74933444 -7.10137084]]\n"
          ]
        }
      ],
      "source": [
        "#here we assume n = 4, and each X, say [1,1,2,0] represents the corresponding counts of each of those words in that sentence.\n",
        "X = np.array([[1, 1, 2, 0], [2, 1, 1, 0], [0, 2, 1, 2], [1, 1, 0, 2]])\n",
        "# we have two classes \n",
        "y = np.array([0, 1, 0, 1])\n",
        "\n",
        "nb = MultinomialNaiveBayes()\n",
        "nb.fit(X, y)\n",
        "print(nb.predict(X))\n",
        "assert np.all(nb.predict(X)==y),\"your predictions are wrong\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDz_PuKexylM"
      },
      "source": [
        "Now let's test the effectivness of this algorithm on a real-world data set. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "QR48XYIoteNl"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Load the 20 Newsgroups dataset\n",
        "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
        "newsgroups_test = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "#look at the below cells and understand the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEiXOOnk1grq"
      },
      "source": [
        "This data set has 20 different types of news"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "tKf5h-6n-Paj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49f067a6-4467-413e-b460-f560750835bf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11314"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "len(newsgroups_train.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "KH8qCeWV1pw3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02c71b2d-bae5-4c69-a54e-3d7a013a478a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
          ]
        }
      ],
      "source": [
        "print(list(newsgroups_train.target_names))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "rMZHDfH608dF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "997cd45e-5747-43e0-f4ff-fe8d34981a47"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is \\nall I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "#let's see a sample\n",
        "newsgroups_train.data[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "L7KkXB0j1KSD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26ba7ba3-e056-4891-8cb6-351172381ea2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7, 'rec.autos')"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "#The target attribute is the integer index of the category\n",
        "newsgroups_train.target[0],list(newsgroups_train.target_names)[newsgroups_train.target[0]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3Kb4O9z9vGP"
      },
      "source": [
        "**Q2: PART A [15 POINTS]**\n",
        "\n",
        "Before proceeding, we need to convert the text into Bag of words format(as we saw in toy example). Sklearn has a built in function for it. Read the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) for it below and fill the missing lines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "aUAbRv3P-miY"
      },
      "outputs": [],
      "source": [
        "# Convert the text data into a bag-of-words representation\n",
        "vectorizer = CountVectorizer()\n",
        "# X_train = vectorizer.fit_transform(newsgroups_train.data)\n",
        "vectorizer = vectorizer.fit(newsgroups_train.data)\n",
        "X_train = vectorizer.transform(newsgroups_train.data)\n",
        "X_test = vectorizer.transform(newsgroups_test.data)\n",
        "# Note both train and test data are newsgroups_train.data,newsgroups_test.data respectively. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(newsgroups_test.data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gw_FE-vHODL8",
        "outputId": "07fd904d-4572-4cad-c74b-69fd3c4a9c43"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZf_SO_kOJyD",
        "outputId": "b6c1e7cf-d052-46f1-d494-83bef5004252"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 9843)\t1\n",
            "  (0, 11174)\t1\n",
            "  (0, 16809)\t1\n",
            "  (0, 17936)\t1\n",
            "  (0, 18915)\t2\n",
            "  (0, 21987)\t1\n",
            "  (0, 23480)\t1\n",
            "  (0, 24160)\t1\n",
            "  (0, 24635)\t1\n",
            "  (0, 25492)\t1\n",
            "  (0, 25590)\t1\n",
            "  (0, 25775)\t4\n",
            "  (0, 30074)\t1\n",
            "  (0, 31990)\t1\n",
            "  (0, 34809)\t1\n",
            "  (0, 34810)\t1\n",
            "  (0, 35974)\t1\n",
            "  (0, 37287)\t1\n",
            "  (0, 37335)\t1\n",
            "  (0, 41715)\t2\n",
            "  (0, 41724)\t1\n",
            "  (0, 41979)\t1\n",
            "  (0, 45885)\t1\n",
            "  (0, 46814)\t1\n",
            "  (0, 48754)\t2\n",
            "  :\t:\n",
            "  (11313, 57131)\t1\n",
            "  (11313, 60560)\t1\n",
            "  (11313, 61975)\t1\n",
            "  (11313, 62086)\t1\n",
            "  (11313, 64435)\t1\n",
            "  (11313, 66242)\t1\n",
            "  (11313, 66857)\t2\n",
            "  (11313, 68080)\t1\n",
            "  (11313, 68409)\t1\n",
            "  (11313, 68997)\t1\n",
            "  (11313, 70066)\t1\n",
            "  (11313, 71786)\t1\n",
            "  (11313, 71992)\t1\n",
            "  (11313, 78365)\t1\n",
            "  (11313, 81742)\t1\n",
            "  (11313, 81792)\t1\n",
            "  (11313, 82660)\t1\n",
            "  (11313, 84605)\t1\n",
            "  (11313, 85524)\t1\n",
            "  (11313, 87730)\t1\n",
            "  (11313, 89465)\t1\n",
            "  (11313, 89804)\t1\n",
            "  (11313, 90644)\t1\n",
            "  (11313, 96497)\t1\n",
            "  (11313, 96707)\t1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eU8MOp3cAW1a"
      },
      "source": [
        "**Q2: PART B [15 POINTS]**\n",
        "\n",
        "If your implementation of all the above parts is correct, you should be able to run the below cell. Fill in the below cell for `accuracy = `and report your result ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYKp9Oz2AnzT"
      },
      "source": [
        "Solution: As we can see the accuracy is 0.46269, which is 46.27%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "HiAMATGcuQOY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "721adbd4-a280-4578-cbbd-df6684f9b2f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(20, 101631)\n",
            "(7532, 101631)\n",
            "[[ -664.2190063   -660.98839638  -716.11578155 ...  -642.17683855\n",
            "   -647.83270231  -674.17707868]\n",
            " [ -716.16664562  -695.82606421  -761.2309516  ...  -686.91320203\n",
            "   -695.86896547  -718.24227467]\n",
            " [  -25.92634477   -28.08759149   -28.66156651 ...   -25.45031238\n",
            "    -25.70026508   -25.77189493]\n",
            " ...\n",
            " [ -913.98117426  -941.67135634 -1006.36207561 ...  -884.6243752\n",
            "   -891.05636163  -928.87706045]\n",
            " [ -898.22186277  -846.10215284  -907.4944566  ...  -883.64265579\n",
            "   -885.83110116  -891.79325943]\n",
            " [ -484.07346958  -530.47563039  -567.74555625 ...  -497.97914034\n",
            "   -490.08440267  -497.09983916]]\n",
            "Accuracy: 0.4626925119490175\n"
          ]
        }
      ],
      "source": [
        "# Fit the multinomial Naive Bayes model on the training data\n",
        "mnb = MultinomialNaiveBayes()\n",
        "mnb.fit(X_train, newsgroups_train.target)\n",
        "\n",
        "# Predict the class labels of the testing samples\n",
        "X_test_new = X_test.toarray()\n",
        "print(mnb.counts.shape)\n",
        "print(X_test.shape)\n",
        "\n",
        "\n",
        "y_pred = mnb.predict(X_test_new)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "\n",
        "# accuracy = np.mean(y_pred == newsgroups_test.target)\n",
        "# accuracy = np.mean(np.allclose(y_pred, newsgroups_test.target))\n",
        "\n",
        "accuracy = np.mean(y_pred == newsgroups_test.target) \n",
        "print(\"Accuracy:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IsGRIlimyi6e",
        "outputId": "f6ba3ecc-2606-44bb-c0d3-0486c6f7cdbf"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([15, 17, 15, 17,  0, 13, 15, 17,  5, 15])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "newsgroups_test.target[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6j3Pdjbmymj2",
        "outputId": "e916ef38-3348-4bb6-eead-45d97ff6da0c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 7,  5,  0, 17, 19, 13, 15, 15,  5,  1])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVtPN5h72WR5"
      },
      "source": [
        "**Q2: PART C [5 Points]**\n",
        "\n",
        "In your code for calculating the counts(`self.counts`) \n",
        "\n",
        "```\n",
        "self.counts[i,:] = np.sum(X_class, axis=0) + 1\n",
        "```\n",
        "\n",
        "```\n",
        " self.counts /= np.sum(self.counts, axis=1).reshape(-1, 1) + n_features\n",
        " ```\n",
        "\n",
        " Why do we add 1 in the numerator and include n_features ? Give a short explanation ?\n",
        "\n",
        " Hint: It's called Laplace smoothing. Figure out why it's being used here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kv_Fgswz_3UC"
      },
      "source": [
        "Solution:\n",
        "The reason we add 1 in the numerator and include n_features is because we want to avoid the zero-counts. If we don't have a word in the class, then the count of this word will be 0. When we calculate log probability of this nonexist word, it will become -inf. In this case we will add 1 to all the count. Same logic for add the n_features, we know that np.sum(self.counts, axis=1).reshape(-1, 1) represent total count of each class. So we want to add n_features to the denominator to avoid zero-counts.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-m5tItVhZfU"
      },
      "source": [
        "# Logistic Regression\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuR9P1Wk6PgX"
      },
      "source": [
        "Consider this data set\n",
        "\n",
        "| Feature 1 | Feature 2 | Class |\n",
        "|-----------|-----------|-------|\n",
        "| 1         | 1         | 0     |\n",
        "| 2.2         | 1.6         | 0     |\n",
        "| 2.5         | 1.8         | 0     |\n",
        "| 2.8         | 1.5         | 0     |\n",
        "| 2.9         | 1.2         | 0     |\n",
        "| 3.0        | 3.0        | 1     |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZHPvPOo5oO8"
      },
      "source": [
        "Consider the above data and train a logistic regression model on this data.\n",
        "\n",
        "Here are the learned weights: \n",
        "> $w_0$=  -4.08673095\n",
        "\n",
        "> $w_1$= 0.33123783\n",
        "\n",
        "> $w_2$= 0.89782125\n",
        "\n",
        ">Here $w_0$ corresponds to the intercept.\n",
        "\n",
        "</br>\n",
        "\n",
        "Here are the equations we discussed for class-conditional probabilities on a data-point $x_i = [x_{i1}, x_{i2}, ..., x_{ip}]$ :\n",
        "\n",
        ">$ P(y=1|\\mathbf{x_i}) = \\frac{1}{1 + e^{z}} $ \n",
        "\n",
        ">$ P(y=0|\\mathbf{x_i}) = \\frac{e^{z}}{1 + e^{z}} $ \n",
        "\n",
        ">Where $z = w_0 + w_1 x_{i1} + w_2 x_{i2} + ... + w_p x_{ip}$\n",
        "\n",
        ">Here $w_0$ is the intercept, $w_1, w_2, ..., w_p$ are the coefficients of the predictor variables $x_{i1}, x_{i2}, ..., x_{ip}$ for the $i$-th training sample, respectively.\n",
        "\n",
        "**Q3: PART A [10 points]**\n",
        "\n",
        "What is the accuracy over training data? (show the steps)\n",
        "\n",
        "**Q3: PART B [10 points]**\n",
        "\n",
        "Is the decision boundary linear or non-linear (5 points)? \n",
        "\n",
        "Please write down the expression for the decision boundary (5 points).\n",
        "\n",
        "**Q3: PART C [10 points]**\n",
        "\n",
        "List all the classifiers we have learned, that are generative classifiers. What are their properties? (5 points)\n",
        "\n",
        "List all the classifiers we have learned that are discriminative classifiers. What are their properties? (5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5T_9mk_U_I8m"
      },
      "source": [
        "Solution:\n",
        "\n",
        "part A:\n",
        "The accuracy will be 1/6, 16.67%. Because due to the calculation all samples should be in class 1. Steps are in the pdf\n",
        "\n",
        "part B:\n",
        "The decision boundary should be linear. The expression of the decision bounary is $z = w_0 + w_1 x_{i1} + w_2 x_{i2} + ... + w_p x_{ip}$, where $w_0$=  -4.08673095, $w_1$= 0.33123783, $w_2$= 0.89782125. So our expression will be z = -4.08673095 + 0.33123783 * $x_{i1}$ + 0.89782125 * $x_{i2}$\n",
        "\n",
        "part C:\n",
        "\n",
        "For generative classifiers we learned: Naive Bayes which assume that all features are conditional independent. Probability table\n",
        "\n",
        "Generative classifiers is about estimate parameters of P(X|Y). The properties of generative classifiers is that we view P(X|Y) as describing how to sample random instance X given Y. Also, after we model the distribution of the input for each class, we can generate new samples for each class.\n",
        "\n",
        "For discriminative classifiers we learned: logistic regression, decision tree\n",
        "\n",
        "Discriminative classifiers is about estimate parameters of P(Y|X). The properties are we do not care how instances are generated. We can also learn a decision boundary directly from training data. Different from the generative classifiers, we cannot generate new samples using discriminative classifiers."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "118d2d70b9837ec74e99cd8e271bd7c1e24309015268c38b027840ab45e80ec0"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}